{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Function\n",
    "\n",
    "from src.ml.sinkhorn import pot_sinkhorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 512\n",
    "n_clusters = 128\n",
    "batch_size = 64\n",
    "n_samples = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, n_features, n_clusters, n_samples):\n",
    "        self.n_features = n_features\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_samples = n_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_true[idx]\n",
    "    \n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(\n",
    "            nrows=1,\n",
    "            ncols=1,\n",
    "            figsize=(5, 5)\n",
    "        )\n",
    "\n",
    "        sns.scatterplot(  # plot first 2 components\n",
    "            x=self.X[:, 0],\n",
    "            y=self.X[:, 1],\n",
    "            hue=map(str, self.y_true),\n",
    "            ax=ax,\n",
    "            legend=False\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(\"Component 1\")\n",
    "        ax.set_ylabel(\"Component 2\")\n",
    "        ax.set_title(\"Clusters visualization\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "\n",
    "class BlobsDataset(ToyDataset):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_clusters, n_samples):\n",
    "        super().__init__(n_features, n_clusters, n_samples)\n",
    "        \n",
    "        X, y_true = make_blobs(\n",
    "            n_samples=n_samples,\n",
    "            n_features=n_features,\n",
    "            centers=n_clusters,\n",
    "            cluster_std=.8,\n",
    "            random_state=0\n",
    "        )\n",
    "        \n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y_true = torch.LongTensor(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BlobsDataset(n_features, n_clusters, n_samples)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        mlp_out = self.mlp(inputs)\n",
    "        softmax_out = nn.LogSoftmax(dim=1)(mlp_out)\n",
    "        \n",
    "        return softmax_out\n",
    "    \n",
    "model = Model(\n",
    "    input_dim=n_features,\n",
    "    output_dim=n_clusters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queue implementation\n",
    "\n",
    "- [x] stored_M (= previous batchs dans la queue)\n",
    "- [x] M = current batch\n",
    "- [x] tu rajoutes stored_M comme argument supplementaire à SinkhornValueFunc\n",
    "- [x] sinkhorn est execute sur M_full = concatenation de M et stored_M\n",
    "- [ ] puis dans SinkhornValue tu implemente la logique de storer les M quand tu call la fonction, avec une logique de queue FIFO \n",
    "- [x] et pas besoin d'utiliser une queue pour cela, juste un tensor me semble suffisant, juste quand tu arrives à la limite de taille, pour inserer le nouveau batch, tu \"roll\" le tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hz/7fxv73md5ddd3sy5z4pdzn1h0000gn/T/ipykernel_23116/2065042902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstored_M\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# tensor acts as queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_n_batches_in_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m  \u001b[0;31m# max number of batches in queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# M is model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "stored_M = torch.Tensor()   # tensor acts as queue\n",
    "max_n_batches_in_queue = 4  # max number of batches in queue\n",
    "\n",
    "for batch_ix, (inputs, labels) in enumerate(dataloader):\n",
    "    # M is model output\n",
    "    M = model(inputs)\n",
    "    \n",
    "    #################\n",
    "    # Sinkhorn step #\n",
    "    #################\n",
    "    \n",
    "    M_concat = torch.cat([M, stored_M])\n",
    "    \n",
    "    # Compute marginals\n",
    "    a = torch.ones(M_concat.shape[0])\n",
    "    b = torch.ones(M_concat.shape[0] / M.shape[1]) / M.shape[1]\n",
    "    \n",
    "    # Compute sinkhorn\n",
    "    P = pot_sinkhorn(M_concat, a, b, epsilon=0.1)\n",
    "\n",
    "    ################ \n",
    "    # Update queue #\n",
    "    ################\n",
    "    \n",
    "    # Update stored M\n",
    "    n_batches_in_queue = stored_M.shape[0] / batch_size\n",
    "    if  n_batches_in_queue < max_n_batches_in_queue:\n",
    "        # Append current batch to previous batches\n",
    "        stored_M = M_concat\n",
    "    else:\n",
    "        # Roll stored M, older batch comes first, replace it with M\n",
    "        stored_M = torch.roll(stored_M, batch_size, 0)  # roll, first batch is the oldest\n",
    "        stored_M[:batch_size, :] = M                    # update first batch with new one\n",
    "    \n",
    "    # Print for debug\n",
    "    print(\"Batch {0}: {1}\".format(batch_ix, stored_M.shape))\n",
    "    \n",
    "    if batch_ix == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queue integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinkhornValueFunc(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, M, stored_M, a, b, epsilon, solver, solver_options):\n",
    "        # Run Sinkhorn\n",
    "        P = solver(\n",
    "            torch.cat([M, stored_M]),  # Use the queue\n",
    "            a,\n",
    "            b,\n",
    "            epsilon,\n",
    "            **solver_options\n",
    "        )\n",
    "        P = P[:M.shape[0], :]  # Take only current batch\n",
    "\n",
    "        ctx.save_for_backward(P)\n",
    "        return (P*M).sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        P, = ctx.saved_tensors\n",
    "        grad_M = P * grad_output\n",
    "\n",
    "        return grad_M, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "class SinkhornValue(nn.Module):\n",
    "    \"\"\"Sinkhorn value.\n",
    "\n",
    "    Returns optimal value for the regularized OT problem:\n",
    "        L(M) = max <M, P> + \\epsilon H[P] s.t. \\sum_j P_ij = a_i and \\sum_i P_ij = b_j\n",
    "    with entropy H[P] = - \\sum_ij P_ij [log(P_ij) - 1]\n",
    "\n",
    "    Args:\n",
    "        epsilon (float): regularization parameter\n",
    "        solver (function): OT solver\n",
    "        solver_kwargs (int): options to pass to the solver\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon, max_n_batches_in_queue, solver, **solver_options):\n",
    "        super().__init__()\n",
    "        # Sinkhorn params\n",
    "        self.epsilon = epsilon\n",
    "        self.solver = solver\n",
    "        self.solver_options = solver_options\n",
    "        \n",
    "        # Queue params\n",
    "        self.stored_M = torch.Tensor()                        # tensor acts as queue\n",
    "        self.max_n_batches_in_queue = max_n_batches_in_queue  # max number of batches in queue\n",
    "\n",
    "    def forward(self, M):\n",
    "        batch_size = M.shape[0]\n",
    "        \n",
    "        #################\n",
    "        # Sinkhorn step #\n",
    "        #################\n",
    "        # Compute marginals\n",
    "        M_concat = torch.cat([M, self.stored_M])\n",
    "        a = torch.ones(M_concat.shape[0])\n",
    "        b = torch.ones(M.shape[1]) / (M.shape[0] / M.shape[1])\n",
    "\n",
    "        # Compute sinkhorn\n",
    "        loss = SinkhornValueFunc.apply(\n",
    "            M,\n",
    "            self.stored_M,\n",
    "            a,\n",
    "            b,\n",
    "            self.epsilon,\n",
    "            self.solver,\n",
    "            self.solver_options\n",
    "        )\n",
    "        \n",
    "        ################\n",
    "        # Update queue #\n",
    "        ################\n",
    "        n_batches_in_queue = self.stored_M.shape[0] / batch_size\n",
    "        if n_batches_in_queue < self.max_n_batches_in_queue:\n",
    "            # Append current batch to previous batches\n",
    "            self.stored_M = M_concat\n",
    "        else:\n",
    "            # Roll stored M, older batch comes first, replace it with M\n",
    "            self.stored_M = torch.roll(self.stored_M, batch_size, 0)  # roll, first batch is the oldest\n",
    "            self.stored_M[:batch_size, :] = M                         # update first batch with new one\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return (\n",
    "            f\"epsilon={self.epsilon:.2e}, solver={self.solver}\"\n",
    "            \"solver_options={self.solver_options}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) tensor(4.2025, grad_fn=<SinkhornValueFuncBackward>)\n",
      "torch.Size([128, 128]) tensor(2.0841, grad_fn=<SinkhornValueFuncBackward>)\n",
      "torch.Size([192, 128]) tensor(1.3816, grad_fn=<SinkhornValueFuncBackward>)\n",
      "torch.Size([256, 128]) tensor(1.0406, grad_fn=<SinkhornValueFuncBackward>)\n",
      "torch.Size([256, 128]) tensor(0.8220, grad_fn=<SinkhornValueFuncBackward>)\n",
      "torch.Size([256, 128]) tensor(0.8202, grad_fn=<SinkhornValueFuncBackward>)\n",
      "torch.Size([256, 128]) tensor(0.8283, grad_fn=<SinkhornValueFuncBackward>)\n",
      "torch.Size([256, 128]) tensor(0.8239, grad_fn=<SinkhornValueFuncBackward>)\n",
      "torch.Size([256, 128]) tensor(0.8215, grad_fn=<SinkhornValueFuncBackward>)\n",
      "torch.Size([256, 128]) tensor(0.8300, grad_fn=<SinkhornValueFuncBackward>)\n",
      "torch.Size([256, 128]) tensor(0.8281, grad_fn=<SinkhornValueFuncBackward>)\n"
     ]
    }
   ],
   "source": [
    "SV = SinkhornValue(\n",
    "    epsilon=0.1,\n",
    "    solver=pot_sinkhorn,\n",
    "    max_n_batches_in_queue=4\n",
    ")\n",
    "\n",
    "for batch_ix, (inputs, labels) in enumerate(dataloader):\n",
    "    M = model(inputs)\n",
    "    \n",
    "    loss = SV(-M)\n",
    "    print(SV.stored_M.shape, loss)\n",
    "    \n",
    "    if batch_ix == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35e7e15e99ced0a59866f00000561dd568615f64817c6500270e1aee9e906325"
  },
  "kernelspec": {
   "display_name": "Python 3 (data-science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
