{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinkhorn loss module\n",
    "\n",
    "\n",
    "Implements `SinkhornLoss` module for\n",
    "\n",
    "$loss = H[target, P(M)]$ where $P(M)$ is the solution of the regularised OT problem with affinity matrix M.\n",
    "\n",
    "It overwrites the backward in the associated `SinkhornLossFunc`.\n",
    "\n",
    "Drawback of this approach: we do not have access to P.\n",
    "\n",
    "**Notation**\n",
    "\n",
    "OT convention for entropy\n",
    "$H[\\pi] = -\\langle \\pi, \\log \\pi -1 \\rangle$\n",
    "\n",
    "Usual definition for cross entropy\n",
    "$H[\\sigma, \\pi] = - \\langle \\sigma, \\log \\pi \\rangle$\n",
    "\n",
    "$\\text{loss} = H[\\sigma, \\pi] = \\frac{1}{\\epsilon} [L_\\epsilon(M) - \\langle \\sigma , M\\rangle ] - \\langle \\sigma, 1\\rangle$\n",
    "\n",
    "$\\nabla_M \\text{loss} = \\frac{1}{\\epsilon} [\\pi - \\sigma ]$\n",
    "\n",
    "\n",
    "\n",
    "**TODO**\n",
    "\n",
    "- add convergence criterion in sinkhorn (tol)\n",
    "- maybe use exernal solver (python OT)\n",
    "- give access to P in module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "\n",
    "def sinkhorn(M, a, b, epsilon, n_iter):\n",
    "    \"\"\"Basic sinkhorn algorithm. \n",
    "    \n",
    "    Solves the regularised OT problem:\n",
    "        max <M, P> + \\epsilon H[P] s.t. \\sum_j P_ij = a_i and \\sum_i P_ij = b_j\n",
    "    with entropy H[P] = - \\sum_ij P_ij [log(P_ij) - 1]\n",
    "    \n",
    "    Args:\n",
    "        M (torch.Tensor): affinity matrix\n",
    "        a (torch.Tensor): user capacities\n",
    "        b (torch.Tensor): item capacities\n",
    "        epsilon (float): regularisation parameter\n",
    "        n_iter (int): number of iteration\n",
    "    \n",
    "    Returns:\n",
    "        P (torch.Tensor): coupling matrix\n",
    "    \"\"\"\n",
    "    v = torch.ones_like(b)\n",
    "    K = torch.exp(M / epsilon)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        u = a / torch.matmul(K, v)\n",
    "        v = b / torch.matmul(torch.transpose(K, 1, 0), u)\n",
    "        \n",
    "    uv = torch.outer(u, v)\n",
    "    P = K * uv\n",
    "    \n",
    "    return P\n",
    "\n",
    "\n",
    "class SinkhornLossFunc(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, M, target, a, b, epsilon, solver, solver_options):\n",
    "        P = solver(M.detach(), a, b, epsilon, **solver_options)\n",
    "        cross_entropy = - (target * P.log()).sum()\n",
    "        delta_P = (P - target) / epsilon\n",
    "        ctx.save_for_backward(delta_P)\n",
    "        \n",
    "        return cross_entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        delta_P, = ctx.saved_tensors\n",
    "        grad_M = delta_P * grad_output\n",
    "        \n",
    "        return grad_M, None, None, None, None, None, None   \n",
    "\n",
    "\n",
    "class SinkhornLoss(nn.Module):\n",
    "    \"\"\"Sinkhorn loss. \n",
    "    \n",
    "    Computes loss = H[target, P(M)] where P(M) is the solution of the regularised OT problem\n",
    "    with affinity matrix M.\n",
    "    \n",
    "    Args:\n",
    "        a (torch.Tensor): user capacities\n",
    "        b (torch.Tensor): item capacities\n",
    "        epsilon (float): regularisation parameter\n",
    "        solver (function): OT solver\n",
    "        solver_kwargs (int): options to pass to the solver\n",
    "    \"\"\"\n",
    "    def __init__(self, a, b, epsilon, solver, **solver_options):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "        self.solver = solver\n",
    "        self.solver_options = solver_options\n",
    "        \n",
    "    def forward(self, M, target):\n",
    "        return SinkhornLossFunc.apply(M, target, self.a, self.b, self.epsilon, self.solver, self.solver_options)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (\n",
    "            f\"a={self.a},\\nb={self.b},\\n\"\n",
    "            f\"epsilon={self.epsilon:.2e}, solver={self.solver}, solver_options={self.solver_options}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# affinity matrix\n",
    "M = torch.rand(3, 6, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "\n",
    "target = torch.rand(*M.shape, dtype=torch.double)\n",
    "a = target.sum(axis=1)\n",
    "b = target.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SinkhornLoss(\n",
       "  a=tensor([2.4401, 1.5512, 2.2407], dtype=torch.float64),\n",
       "  b=tensor([0.9066, 0.7111, 1.2323, 1.1531, 1.1917, 1.0373], dtype=torch.float64),\n",
       "  epsilon=1.00e-02, solver=<function sinkhorn at 0x13cba4820>, solver_options={'n_iter': 100}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SL = SinkhornLoss(a ,b, 0.01, sinkhorn, n_iter=100)\n",
    "SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-62.3403, -43.7279,  26.2985, -68.3430,  87.9453,  60.2457],\n",
       "        [ 87.4850,  60.3409, -93.1909, -15.2126, -26.5045, -13.0433],\n",
       "        [-25.1447, -16.6130,  66.8924,  83.5556, -61.4408, -47.2023]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = SL(M, target)\n",
    "M.grad = None\n",
    "loss.backward()\n",
    "M.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradcheck pass for epsilon=0.10\n",
      "gradcheck pass for epsilon=0.09\n",
      "gradcheck pass for epsilon=0.08\n",
      "gradcheck pass for epsilon=0.07\n",
      "gradcheck pass for epsilon=0.06\n",
      "gradcheck pass for epsilon=0.05\n",
      "gradcheck error for epsilon=0.04\n"
     ]
    }
   ],
   "source": [
    "# alias\n",
    "sinkhorn_loss = SinkhornLossFunc.apply\n",
    "# gradcheck do not pass for too small epsilon\n",
    "from torch.autograd import gradcheck\n",
    "import numpy as np\n",
    "for epsilon in np.linspace(0.1,0,11):\n",
    "    inputs = (M, target, a, b, epsilon, sinkhorn, dict(n_iter=100))\n",
    "    try:\n",
    "        gradcheck(sinkhorn_loss, inputs)\n",
    "    except:\n",
    "        print(f\"gradcheck error for epsilon={epsilon:.2f}\")\n",
    "        break\n",
    "    print(f\"gradcheck pass for epsilon={epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinkhorn value module\n",
    "\n",
    "\n",
    "Implements `SinkhornValue` module that takes the affinity matrix $M$ as input and returns the value function of the regularised OT problem:\n",
    "\n",
    "$L(M) = L_M^\\epsilon(a,b) = \\max_{\\pi \\in U(a,b)} \\langle \\pi, M \\rangle + \\epsilon H[\\pi]$ \n",
    "\n",
    "The prescribed marginals $a$ and $b$ are given parameters, \n",
    "we take the OT convention for entropy\n",
    "\n",
    "$H[\\pi] = -\\langle \\pi, \\log \\pi -1 \\rangle$\n",
    "\n",
    "It overwrites the backward in the associated `SinkhornValueFunc`.\n",
    "\n",
    "The gradient wrt $M$ is simply (see Prop 9.2 in Computational OT, Peyr√© and Cuturi):\n",
    "\n",
    "$\\nabla_M L_M^\\epsilon(a,b) = \\pi^*$\n",
    "\n",
    "where $\\pi^*$ is the optimal coupling:\n",
    "\n",
    "$\\pi^* = \\arg\\max_{\\pi \\in U(a,b)} \\langle \\pi, M \\rangle + \\epsilon H[\\pi]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinkhornValueFunc(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, M, a, b, epsilon, solver, solver_options):\n",
    "        P = solver(M.detach(), a, b, epsilon, **solver_options)\n",
    "        ctx.save_for_backward(P)\n",
    "        # clamping log(P) to -100 to avoid 0 log(0) = nan\n",
    "        log_P = P.log().clamp(min=-100)\n",
    "        H = (P * (1 - log_P)).sum()\n",
    "        value_OT = (P*M).sum() + epsilon*H\n",
    "        return value_OT\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        P, = ctx.saved_tensors\n",
    "        grad_M = P * grad_output\n",
    "        return grad_M, None, None, None, None, None   \n",
    "\n",
    "\n",
    "class SinkhornValue(nn.Module):\n",
    "    \"\"\"Sinkhorn value. \n",
    "    \n",
    "    Returns optimal value for the regularised OT problem:\n",
    "        L(M) = max <M, P> + \\epsilon H[P] s.t. \\sum_j P_ij = a_i and \\sum_i P_ij = b_j\n",
    "    with entropy H[P] = - \\sum_ij P_ij [log(P_ij) - 1]\n",
    "    \n",
    "    Args:\n",
    "        a (torch.Tensor): user capacities\n",
    "        b (torch.Tensor): item capacities\n",
    "        epsilon (float): regularisation parameter\n",
    "        solver (function): OT solver\n",
    "        solver_kwargs (int): options to pass to the solver\n",
    "    \"\"\"\n",
    "    def __init__(self, a, b, epsilon, solver, **solver_options):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "        self.solver = solver\n",
    "        self.solver_options = solver_options\n",
    "        \n",
    "    def forward(self, M):\n",
    "        return SinkhornValueFunc.apply(M, self.a, self.b, self.epsilon, self.solver, self.solver_options)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (\n",
    "            f\"a={self.a},\\nb={self.b},\\n\"\n",
    "            f\"epsilon={self.epsilon:.2e}, solver={self.solver}, solver_options={self.solver_options}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([6]), torch.Size([3, 6]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape, M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SinkhornValue(\n",
       "  a=tensor([2.4401, 1.5512, 2.2407], dtype=torch.float64),\n",
       "  b=tensor([0.9066, 0.7111, 1.2323, 1.1531, 1.1917, 1.0373], dtype=torch.float64),\n",
       "  epsilon=1.00e-02, solver=<function sinkhorn at 0x13cba4820>, solver_options={'n_iter': 100}\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SV = SinkhornValue(a ,b, 0.01, sinkhorn, n_iter=100)\n",
    "SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4196e-27, 2.2927e-26, 3.3772e-01, 2.2559e-14, 1.1917e+00, 9.1152e-01],\n",
       "        [9.0619e-01, 6.4378e-01, 4.3136e-15, 4.7384e-23, 7.7781e-29, 2.5152e-17],\n",
       "        [4.0832e-04, 6.7278e-02, 8.9459e-01, 1.1531e+00, 2.7904e-30, 1.2575e-01]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = SV(M)\n",
    "M.grad = None\n",
    "loss.backward()\n",
    "M.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8302, dtype=torch.float64, grad_fn=<SinkhornValueFuncBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradcheck pass for epsilon=0.10\n",
      "gradcheck pass for epsilon=0.09\n",
      "gradcheck pass for epsilon=0.08\n",
      "gradcheck pass for epsilon=0.07\n",
      "gradcheck pass for epsilon=0.06\n",
      "gradcheck error for epsilon=0.05\n"
     ]
    }
   ],
   "source": [
    "# alias\n",
    "sinkhorn_value = SinkhornValueFunc.apply\n",
    "# gradcheck do not pass for too small epsilon\n",
    "from torch.autograd import gradcheck\n",
    "import numpy as np\n",
    "for epsilon in np.linspace(0.1,0,11):\n",
    "    inputs = (M, a, b, epsilon, sinkhorn, dict(n_iter=100))\n",
    "    try:\n",
    "        gradcheck(sinkhorn_value, inputs)\n",
    "    except:\n",
    "        print(f\"gradcheck error for epsilon={epsilon:.2f}\")\n",
    "        break\n",
    "    print(f\"gradcheck pass for epsilon={epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_env",
   "language": "python",
   "name": "pt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
