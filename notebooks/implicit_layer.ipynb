{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinkhorn loss module\n",
    "\n",
    "\n",
    "Implements `SinkhornLoss` module for\n",
    "\n",
    "$loss = H[target, P(M)]$ where $P(M)$ is the solution of the regularised OT problem with affinity matrix M.\n",
    "\n",
    "It overwrites the backward in the associated `SinkhornLossFunc`.\n",
    "\n",
    "Drawback of this approach: we do not have access to P.\n",
    "\n",
    "**Notation**\n",
    "\n",
    "OT convention for entropy\n",
    "$H[\\pi] = -\\langle \\pi, \\log \\pi -1 \\rangle$\n",
    "\n",
    "Usual definition for cross entropy\n",
    "$H[\\sigma, \\pi] = - \\langle \\sigma, \\log \\pi \\rangle$\n",
    "\n",
    "$\\text{loss} = H[\\sigma, \\pi] = \\frac{1}{\\epsilon} [L_\\epsilon(M) - \\langle \\sigma , M\\rangle ] - \\langle \\sigma, 1\\rangle$\n",
    "\n",
    "$\\nabla_M \\text{loss} = \\frac{1}{\\epsilon} [\\pi - \\sigma ]$\n",
    "\n",
    "\n",
    "\n",
    "**TODO**\n",
    "\n",
    "- add convergence criterion in sinkhorn (tol)\n",
    "- maybe use exernal solver (python OT)\n",
    "- give access to P in module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "\n",
    "def sinkhorn(M, a, b, epsilon, n_iter):\n",
    "    \"\"\"Basic sinkhorn algorithm. \n",
    "    \n",
    "    Solves the regularised OT problem:\n",
    "        max <M, P> + \\epsilon H[P] s.t. \\sum_j P_ij = a_i and \\sum_i P_ij = b_j\n",
    "    with entropy H[P] = - \\sum_ij P_ij [log(P_ij) - 1]\n",
    "    \n",
    "    Args:\n",
    "        M (torch.Tensor): affinity matrix\n",
    "        a (torch.Tensor): user capacities\n",
    "        b (torch.Tensor): item capacities\n",
    "        epsilon (float): regularisation parameter\n",
    "        n_iter (int): number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        P (torch.Tensor): coupling matrix\n",
    "    \"\"\"\n",
    "    v = torch.ones_like(b)\n",
    "    K = torch.exp(M / epsilon)\n",
    "    for _ in range(n_iter):\n",
    "        u = a / torch.matmul(K, v)\n",
    "        v = b / torch.matmul(torch.transpose(K, 1, 0), u)\n",
    "    uv = torch.outer(u, v)\n",
    "    P = K * uv\n",
    "    return P\n",
    "\n",
    "\n",
    "class SinkhornLossFunc(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, M, target, a, b, epsilon, solver, solver_options):\n",
    "        P = solver(M.detach(), a, b, epsilon, **solver_options)\n",
    "        cross_entropy = -(target*P.log()).sum()\n",
    "        delta_P = (P - target)/epsilon\n",
    "        ctx.save_for_backward(delta_P)\n",
    "        return cross_entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        delta_P, = ctx.saved_tensors\n",
    "        grad_M = delta_P*grad_output\n",
    "        return grad_M, None, None, None, None, None, None   \n",
    "\n",
    "# alias\n",
    "sinkhorn_loss = SinkhornLossFunc.apply\n",
    "\n",
    "\n",
    "class SinkhornLoss(nn.Module):\n",
    "    \"\"\"Sinkhorn loss. \n",
    "    \n",
    "    Computes loss = H[target, P(M)] where P(M) is the solution of the regularised OT problem\n",
    "    with affinity matrix M.\n",
    "    \n",
    "    Args:\n",
    "        a (torch.Tensor): user capacities\n",
    "        b (torch.Tensor): item capacities\n",
    "        epsilon (float): regularisation parameter\n",
    "        solver (function): OT solver\n",
    "        solver_kwargs (int): options to pass to the solver\n",
    "    \"\"\"\n",
    "    def __init__(self, a, b, epsilon, solver, **solver_options):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "        self.solver = solver\n",
    "        self.solver_options = solver_options\n",
    "        \n",
    "    def forward(self, M, target):\n",
    "        return sinkhorn_loss(M, target, self.a, self.b, self.epsilon, self.solver, self.solver_options)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (\n",
    "            f\"a={self.a},\\nb={self.b},\\n\"\n",
    "            f\"epsilon={self.epsilon:.2e}, solver={self.solver}, solver_options={self.solver_options}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "M = torch.rand(3,6, dtype=torch.double, requires_grad=True)\n",
    "target = torch.rand(3,6, dtype=torch.double)\n",
    "a = target.sum(axis=1)\n",
    "b = target.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SinkhornLoss(\n",
       "  a=tensor([2.4401, 1.5512, 2.2407], dtype=torch.float64),\n",
       "  b=tensor([0.9066, 0.7111, 1.2323, 1.1531, 1.1917, 1.0373], dtype=torch.float64),\n",
       "  epsilon=1.00e-02, solver=<function sinkhorn at 0x13c3b09d0>, solver_options={'n_iter': 100}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SL = SinkhornLoss(a,b,0.01,sinkhorn,n_iter=100)\n",
    "SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-62.3403, -43.7279,  26.2985, -68.3430,  87.9453,  60.2457],\n",
       "        [ 87.4850,  60.3409, -93.1909, -15.2126, -26.5045, -13.0433],\n",
       "        [-25.1447, -16.6130,  66.8924,  83.5556, -61.4408, -47.2023]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = SL(M, target)\n",
    "M.grad = None\n",
    "loss.backward(retain_graph=True)\n",
    "M.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradcheck pass for epsilon=0.10\n",
      "gradcheck pass for epsilon=0.09\n",
      "gradcheck pass for epsilon=0.08\n",
      "gradcheck pass for epsilon=0.07\n",
      "gradcheck pass for epsilon=0.06\n",
      "gradcheck pass for epsilon=0.05\n"
     ]
    },
    {
     "ename": "GradcheckError",
     "evalue": "Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-15.5851],\n        [-10.9320],\n        [ 12.5324],\n        [-17.0737],\n        [ 21.9863],\n        [  9.0720],\n        [ 21.4451],\n        [ 15.5021],\n        [-23.2657],\n        [ -3.8026],\n        [ -6.6261],\n        [ -3.2528],\n        [ -5.8600],\n        [ -4.5702],\n        [ 10.7333],\n        [ 20.8763],\n        [-15.3602],\n        [ -5.8193]], dtype=torch.float64)\nanalytical:tensor([[-15.5851],\n        [-10.9320],\n        [ 12.5354],\n        [-17.0737],\n        [ 21.9863],\n        [  9.0743],\n        [ 21.4427],\n        [ 15.4957],\n        [-23.2659],\n        [ -3.8026],\n        [ -6.6261],\n        [ -3.2528],\n        [ -5.8576],\n        [ -4.5637],\n        [ 10.7305],\n        [ 20.8763],\n        [-15.3602],\n        [ -5.8215]], dtype=torch.float64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGradcheckError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fp/n1s_x82n33l1bpkl0t8x4q3m0000gn/T/ipykernel_2739/1669510784.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msinkhorn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgradcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msinkhorn_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"gradcheck pass for epsilon={epsilon:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt_env/lib/python3.9/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_forward_ad, fast_mode)\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_gradcheck_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt_env/lib/python3.9/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36m_gradcheck_helper\u001b[0;34m(func, inputs, eps, atol, rtol, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_forward_ad, fast_mode)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[0mgradcheck_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fast_gradcheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfast_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_slow_gradcheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m     _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,\n\u001b[0m\u001b[1;32m   1277\u001b[0m                          rtol, atol, check_grad_dtypes, check_forward_ad=check_forward_ad, nondet_tol=nondet_tol)\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt_env/lib/python3.9/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36m_gradcheck_real_imag\u001b[0;34m(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, check_forward_ad, nondet_tol)\u001b[0m\n\u001b[1;32m    944\u001b[0m                      rtol, atol, check_grad_dtypes, nondet_tol, complex_indices=complex_out_indices)\n\u001b[1;32m    945\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,\n\u001b[0m\u001b[1;32m    947\u001b[0m                      rtol, atol, check_grad_dtypes, nondet_tol)\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt_env/lib/python3.9/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36m_slow_gradcheck\u001b[0;34m(func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, nondet_tol, use_forward_ad, complex_indices, test_imag)\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalytical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_allclose_with_type_promotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mGradcheckError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_notallclose_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_imag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGradcheckError\u001b[0m: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-15.5851],\n        [-10.9320],\n        [ 12.5324],\n        [-17.0737],\n        [ 21.9863],\n        [  9.0720],\n        [ 21.4451],\n        [ 15.5021],\n        [-23.2657],\n        [ -3.8026],\n        [ -6.6261],\n        [ -3.2528],\n        [ -5.8600],\n        [ -4.5702],\n        [ 10.7333],\n        [ 20.8763],\n        [-15.3602],\n        [ -5.8193]], dtype=torch.float64)\nanalytical:tensor([[-15.5851],\n        [-10.9320],\n        [ 12.5354],\n        [-17.0737],\n        [ 21.9863],\n        [  9.0743],\n        [ 21.4427],\n        [ 15.4957],\n        [-23.2659],\n        [ -3.8026],\n        [ -6.6261],\n        [ -3.2528],\n        [ -5.8576],\n        [ -4.5637],\n        [ 10.7305],\n        [ 20.8763],\n        [-15.3602],\n        [ -5.8215]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# gradcheck do not pass for too small epsilon\n",
    "from torch.autograd import gradcheck\n",
    "import numpy as np\n",
    "for epsilon in np.linspace(0.1,0,11):\n",
    "    inputs = (M, target, a, b, epsilon, sinkhorn, dict(n_iter=100))\n",
    "    gradcheck(sinkhorn_loss, inputs)\n",
    "    print(f\"gradcheck pass for epsilon={epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_env",
   "language": "python",
   "name": "pt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
